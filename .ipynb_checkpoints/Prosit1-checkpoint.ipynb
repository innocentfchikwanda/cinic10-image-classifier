{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4376ab55-a8e0-4565-8e3a-1a1f8347d7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"CINIC-10 MLP Pipeline (Colab-ready)\n",
    "\n",
    "Paste this script into Google Colab as alternating Markdown and Code cells.\n",
    "Install deps in a separate Colab cell:\n",
    "  !pip install -q torch torchvision tensorboard matplotlib scikit-learn seaborn wandb\n",
    "\"\"\"\n",
    "\n",
    "# Colab guard (safe outside Colab)\n",
    "try:\n",
    "    from google.colab import drive, userdata  # type: ignore\n",
    "    drive.mount('/content/drive')\n",
    "except Exception:\n",
    "    drive = None\n",
    "    userdata = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6316e548-49f7-439b-a444-04eb9c4cc36c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /Users/innocentchikwanda/miniconda3/envs/d2l/lib/python3.9/site-packages (4.67.1)\n",
      "Requirement already satisfied: wandb in /Users/innocentchikwanda/miniconda3/envs/d2l/lib/python3.9/site-packages (0.22.0)\n",
      "Requirement already satisfied: tensorboard in /Users/innocentchikwanda/miniconda3/envs/d2l/lib/python3.9/site-packages (2.20.0)\n",
      "Requirement already satisfied: click>=8.0.1 in /Users/innocentchikwanda/miniconda3/envs/d2l/lib/python3.9/site-packages (from wandb) (8.1.8)\n",
      "Requirement already satisfied: eval-type-backport in /Users/innocentchikwanda/miniconda3/envs/d2l/lib/python3.9/site-packages (from wandb) (0.2.2)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /Users/innocentchikwanda/miniconda3/envs/d2l/lib/python3.9/site-packages (from wandb) (3.1.45)\n",
      "Requirement already satisfied: packaging in /Users/innocentchikwanda/miniconda3/envs/d2l/lib/python3.9/site-packages (from wandb) (24.2)\n",
      "Requirement already satisfied: platformdirs in /Users/innocentchikwanda/miniconda3/envs/d2l/lib/python3.9/site-packages (from wandb) (4.3.6)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /Users/innocentchikwanda/miniconda3/envs/d2l/lib/python3.9/site-packages (from wandb) (6.32.1)\n",
      "Requirement already satisfied: pydantic<3 in /Users/innocentchikwanda/miniconda3/envs/d2l/lib/python3.9/site-packages (from wandb) (2.11.9)\n",
      "Requirement already satisfied: pyyaml in /Users/innocentchikwanda/miniconda3/envs/d2l/lib/python3.9/site-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /Users/innocentchikwanda/miniconda3/envs/d2l/lib/python3.9/site-packages (from wandb) (2.31.0)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /Users/innocentchikwanda/miniconda3/envs/d2l/lib/python3.9/site-packages (from wandb) (2.38.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.8 in /Users/innocentchikwanda/miniconda3/envs/d2l/lib/python3.9/site-packages (from wandb) (4.12.2)\n",
      "Requirement already satisfied: absl-py>=0.4 in /Users/innocentchikwanda/miniconda3/envs/d2l/lib/python3.9/site-packages (from tensorboard) (2.3.1)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /Users/innocentchikwanda/miniconda3/envs/d2l/lib/python3.9/site-packages (from tensorboard) (1.75.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/innocentchikwanda/miniconda3/envs/d2l/lib/python3.9/site-packages (from tensorboard) (3.9)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /Users/innocentchikwanda/miniconda3/envs/d2l/lib/python3.9/site-packages (from tensorboard) (1.23.5)\n",
      "Requirement already satisfied: pillow in /Users/innocentchikwanda/miniconda3/envs/d2l/lib/python3.9/site-packages (from tensorboard) (11.1.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/innocentchikwanda/miniconda3/envs/d2l/lib/python3.9/site-packages (from tensorboard) (75.8.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/innocentchikwanda/miniconda3/envs/d2l/lib/python3.9/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/innocentchikwanda/miniconda3/envs/d2l/lib/python3.9/site-packages (from tensorboard) (3.1.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/innocentchikwanda/miniconda3/envs/d2l/lib/python3.9/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/innocentchikwanda/miniconda3/envs/d2l/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard) (8.6.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/innocentchikwanda/miniconda3/envs/d2l/lib/python3.9/site-packages (from pydantic<3->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/innocentchikwanda/miniconda3/envs/d2l/lib/python3.9/site-packages (from pydantic<3->wandb) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/innocentchikwanda/miniconda3/envs/d2l/lib/python3.9/site-packages (from pydantic<3->wandb) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/innocentchikwanda/miniconda3/envs/d2l/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/innocentchikwanda/miniconda3/envs/d2l/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/innocentchikwanda/miniconda3/envs/d2l/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/innocentchikwanda/miniconda3/envs/d2l/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/innocentchikwanda/miniconda3/envs/d2l/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/innocentchikwanda/miniconda3/envs/d2l/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "Requirement already satisfied: zipp>=3.20 in /Users/innocentchikwanda/miniconda3/envs/d2l/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard) (3.21.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm wandb tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58bec0b8-7271-4939-a025-4e9146a5f265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (If running this .py outside Colab, comment out the pip lines above.)\n",
    "\n",
    "import os, random, json, copy, math\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset, random_split, ConcatDataset\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7f01598-477e-4bc3-a8e2-41f8ae7260ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "DATA_DIR: data\n",
      "Train exists? True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"# Setup and Paths\n",
    "\n",
    "Config values and output directories. Adjust `FOLDER_PATH` to your CINIC-10 root.\n",
    "\"\"\"\n",
    "# -------------------------\n",
    "# Config / Paths\n",
    "# -------------------------\n",
    "wandb_project = os.environ.get(\"WANDB_PROJECT\", \"cinic10-mlp\")\n",
    "\n",
    "# Point directly to cinic-10\n",
    "FOLDER_PATH = \"./data\"\n",
    "DATA_DIR = Path(FOLDER_PATH)\n",
    "OUT_DIR = Path(\"./Output\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "print(\"DATA_DIR:\", DATA_DIR)\n",
    "print(\"Train exists?\", (DATA_DIR / \"train\").exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "743bc2c4-4d1a-4125-be48-7c14ed1a1718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# W&B helper: only log if a run is active\n",
    "def wandb_log_safe(data: dict):\n",
    "    try:\n",
    "        if wandb.run is not None:\n",
    "            wandb.log(data)\n",
    "    except Exception as e:\n",
    "        print(f\"W&B logging skipped: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88f78226-df9a-4489-992a-6f0585f47ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset exploration summary:\n",
      "{\n",
      "  \"train\": {\n",
      "    \"exists\": true,\n",
      "    \"total_images\": 90000,\n",
      "    \"num_classes\": 10,\n",
      "    \"per_class\": {\n",
      "      \"airplane\": 9000,\n",
      "      \"automobile\": 9000,\n",
      "      \"bird\": 9000,\n",
      "      \"cat\": 9000,\n",
      "      \"deer\": 9000,\n",
      "      \"dog\": 9000,\n",
      "      \"frog\": 9000,\n",
      "      \"horse\": 9000,\n",
      "      \"ship\": 9000,\n",
      "      \"truck\": 9000\n",
      "    },\n",
      "    \"classes\": [\n",
      "      \"airplane\",\n",
      "      \"automobile\",\n",
      "      \"bird\",\n",
      "      \"cat\",\n",
      "      \"deer\",\n",
      "      \"dog\",\n",
      "      \"frog\",\n",
      "      \"horse\",\n",
      "      \"ship\",\n",
      "      \"truck\"\n",
      "    ]\n",
      "  },\n",
      "  \"valid\": {\n",
      "    \"exists\": true,\n",
      "    \"total_images\": 90000,\n",
      "    \"num_classes\": 10,\n",
      "    \"per_class\": {\n",
      "      \"airplane\": 9000,\n",
      "      \"automobile\": 9000,\n",
      "      \"bird\": 9000,\n",
      "      \"cat\": 9000,\n",
      "      \"deer\": 9000,\n",
      "      \"dog\": 9000,\n",
      "      \"frog\": 9000,\n",
      "      \"horse\": 9000,\n",
      "      \"ship\": 9000,\n",
      "      \"truck\": 9000\n",
      "    },\n",
      "    \"classes\": [\n",
      "      \"airplane\",\n",
      "      \"automobile\",\n",
      "      \"bird\",\n",
      "      \"cat\",\n",
      "      \"deer\",\n",
      "      \"dog\",\n",
      "      \"frog\",\n",
      "      \"horse\",\n",
      "      \"ship\",\n",
      "      \"truck\"\n",
      "    ]\n",
      "  },\n",
      "  \"test\": {\n",
      "    \"exists\": true,\n",
      "    \"total_images\": 90000,\n",
      "    \"num_classes\": 10,\n",
      "    \"per_class\": {\n",
      "      \"airplane\": 9000,\n",
      "      \"automobile\": 9000,\n",
      "      \"bird\": 9000,\n",
      "      \"cat\": 9000,\n",
      "      \"deer\": 9000,\n",
      "      \"dog\": 9000,\n",
      "      \"frog\": 9000,\n",
      "      \"horse\": 9000,\n",
      "      \"ship\": 9000,\n",
      "      \"truck\": 9000\n",
      "    },\n",
      "    \"classes\": [\n",
      "      \"airplane\",\n",
      "      \"automobile\",\n",
      "      \"bird\",\n",
      "      \"cat\",\n",
      "      \"deer\",\n",
      "      \"dog\",\n",
      "      \"frog\",\n",
      "      \"horse\",\n",
      "      \"ship\",\n",
      "      \"truck\"\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"# 1) Data Exploration\n",
    "\n",
    "Quickly inspect the CINIC-10 directory structure and counts per split/class.\n",
    "\"\"\"\n",
    "# -------------------------\n",
    "# 1) Data exploration (quick checks)\n",
    "# -------------------------\n",
    "def explore_dataset(base_dir):\n",
    "    info = {}\n",
    "    for split in [\"train\", \"valid\", \"test\"]:\n",
    "        p = Path(base_dir) / split\n",
    "        if not p.exists():\n",
    "            info[split] = {\"exists\": False}\n",
    "            continue\n",
    "        classes = sorted([d.name for d in p.iterdir() if d.is_dir()])\n",
    "        counts = {c: len(list((p/c).glob(\"*\"))) for c in classes}\n",
    "        total = sum(counts.values())\n",
    "        info[split] = {\"exists\": True, \"total_images\": total, \"num_classes\": len(classes), \"per_class\": counts, \"classes\": classes}\n",
    "    return info\n",
    "\n",
    "expl = explore_dataset(DATA_DIR)\n",
    "print(\"Dataset exploration summary:\")\n",
    "print(json.dumps(expl, indent=2)[:2000])  # truncated print\n",
    "\n",
    "# Known dataset facts (CINIC-10): 270,000 images; 90k per split; 32x32; 3 channels (RGB).\n",
    "# If your local dataset differs, expl above will show counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1af511d6-1845-48c3-a4a1-8a5622f2cbd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded datasets sizes -> train: 90000 valid: 90000 test: 90000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"# 2) Preprocessing & Augmentations\n",
    "\n",
    "Normalization (CINIC-10 statistics) and light augmentations for training.\n",
    "\"\"\"\n",
    "# -------------------------\n",
    "# 2) Preprocessing & augmentations\n",
    "# -------------------------\n",
    "# CINIC-10 recommended normalization (common practice). Adjust if desired.\n",
    "MEAN = [0.47889522, 0.47227842, 0.43047404]\n",
    "STD  = [0.24205776, 0.23828046, 0.25874835]\n",
    "\n",
    "train_transform_aug = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.02),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(MEAN, STD),\n",
    "])\n",
    "\n",
    "eval_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(MEAN, STD),\n",
    "])\n",
    "\n",
    "# Load ImageFolder datasets (expects directory/classname structure)\n",
    "train_ds = datasets.ImageFolder(os.path.join(DATA_DIR, \"train\"), transform=train_transform_aug)\n",
    "valid_ds = datasets.ImageFolder(os.path.join(DATA_DIR, \"valid\"), transform=eval_transform)\n",
    "test_ds  = datasets.ImageFolder(os.path.join(DATA_DIR, \"test\"), transform=eval_transform)\n",
    "\n",
    "print(\"Loaded datasets sizes -> train:\", len(train_ds), \"valid:\", len(valid_ds), \"test:\", len(test_ds))\n",
    "CLASS_NAMES = train_ds.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a87932e-2f1f-4232-8c1e-a56db5040861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Small subset size (30% of train): 27000\n",
      "Small splits: 9000 9000 9000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"# 2b) 30% Small Subset\n",
    "\n",
    "Create a 30% random subset of the training data and split it equally into\n",
    "train/eval/test slices (~10% each) for quick experimentation and grid search.\n",
    "\"\"\"\n",
    "# -------------------------\n",
    "# 2b) Create small subset: randomly select 30% of training data (will be split into three equal parts = 10% each)\n",
    "# -------------------------\n",
    "SMALL_SUBSET_FRAC = 0.30\n",
    "small_n = int(len(train_ds) * SMALL_SUBSET_FRAC)\n",
    "all_indices = list(range(len(train_ds)))\n",
    "random.seed(42)\n",
    "random.shuffle(all_indices)\n",
    "small_indices = all_indices[:small_n]\n",
    "small_subset = Subset(train_ds, small_indices)\n",
    "print(\"Small subset size (30% of train):\", len(small_subset))\n",
    "\n",
    "# Split small_subset into three equal parts (each ~10% of original train)\n",
    "third = len(small_subset) // 3\n",
    "lens = [third, third, len(small_subset) - 2*third]\n",
    "small_train_subset, small_eval_subset, small_test_subset = random_split(small_subset, lens)\n",
    "print(\"Small splits:\", len(small_train_subset), len(small_eval_subset), len(small_test_subset))\n",
    "\n",
    "# Convenience: function to get x_train,y_train arrays if needed\n",
    "def subset_to_xy(subset):\n",
    "    # Returns numpy arrays (X flattened not returned here because images are stored in dataset)\n",
    "    X_idx = [subset.indices[i] if isinstance(subset, Subset) else i for i in range(len(subset))]\n",
    "    y = [subset.dataset.samples[idx][1] if isinstance(subset, Subset) else subset.dataset.samples[idx][1] for idx in X_idx]\n",
    "    return X_idx, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cda6a7c-0df2-4602-a4b1-09afa29f52fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# 3) DataLoaders\n",
    "\n",
    "Helpers to build DataLoader objects for the small subset and the final combined set.\n",
    "\"\"\"\n",
    "# -------------------------\n",
    "# 3) DataLoaders: helper\n",
    "# -------------------------\n",
    "def make_dataloaders(batch_size, use_aug_on_full_train=False):\n",
    "    # For small subset (used in grid search)\n",
    "    train_loader = DataLoader(small_train_subset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "    eval_loader  = DataLoader(small_eval_subset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "    test_loader  = DataLoader(small_test_subset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "    return train_loader, eval_loader, test_loader\n",
    "\n",
    "# Full train loaders (for final training)\n",
    "def make_full_train_loader(batch_size, augment=True):\n",
    "    if augment:\n",
    "        # re-create ImageFolder with augmentation for both train and valid to increase diversity on final combined training\n",
    "        train_aug = datasets.ImageFolder(os.path.join(DATA_DIR, \"train\"), transform=train_transform_aug)\n",
    "        valid_aug = datasets.ImageFolder(os.path.join(DATA_DIR, \"valid\"), transform=train_transform_aug)\n",
    "        combined = ConcatDataset([train_aug, valid_aug])\n",
    "    else:\n",
    "        combined = ConcatDataset([train_ds, valid_ds])\n",
    "    return DataLoader(combined, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "\n",
    "test_loader_full = DataLoader(test_ds, batch_size=256, shuffle=False, num_workers=2, pin_memory=True)\n",
    "valid_loader_full = DataLoader(valid_ds, batch_size=256, shuffle=False, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eeb680b4-0010-42e1-874e-a908afb086c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"# 4) Model: Feedforward MLP\n",
    "\n",
    "Fully-connected network with ReLU and Dropout. Output layer is logits for 10 classes.\n",
    "CrossEntropyLoss applies softmax during loss computation.\n",
    "\"\"\"\n",
    "# -------------------------\n",
    "# 4) Model: L-layer feedforward network (MLP)\n",
    "# -------------------------\n",
    "class FeedForwardMLP(nn.Module):\n",
    "    def __init__(self, input_size=3*32*32, hidden_sizes=[2048, 1024, 512, 256], num_classes=10, dropout=0.5):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        in_dim = input_size\n",
    "        for h in hidden_sizes:\n",
    "            layers.append(nn.Linear(in_dim, h))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            in_dim = h\n",
    "        layers.append(nn.Linear(in_dim, num_classes))  # logits (CrossEntropyLoss applies softmax)\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # flatten\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce584777-a038-4638-9eef-4344448e18f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Regularization\n",
    "\n",
    "L1 penalty helper (in addition to L2 via weight decay in the optimizer).\n",
    "\"\"\"\n",
    "# -------------------------\n",
    "# Regularization helpers (L1)\n",
    "# -------------------------\n",
    "def l1_penalty(model):\n",
    "    l1 = torch.tensor(0., device=device)\n",
    "    for p in model.parameters():\n",
    "        l1 += torch.norm(p, 1)\n",
    "    return l1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98df6b1a-31d9-472e-9885-c4c85e9c6ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"# 5) Training and Evaluation Utilities\n",
    "\n",
    "Training loop with optional L1, early stopping (by patience), TensorBoard logging,\n",
    "and W&B metric logging. `evaluate()` computes and logs metrics and confusion matrices.\n",
    "\"\"\"\n",
    "# -------------------------\n",
    "# Training / Eval loops + Early Stopping + TensorBoard logging\n",
    "# -------------------------\n",
    "def train_epoch(model, optimizer, criterion, dataloader, l1_lambda=0.0):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    y_true, y_pred = [], []\n",
    "    for X, y in dataloader:\n",
    "        X = X.to(device); y = y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(X)\n",
    "        loss = criterion(logits, y)\n",
    "        if l1_lambda > 0:\n",
    "            loss = loss + l1_lambda * l1_penalty(model)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * X.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        y_true.extend(y.cpu().numpy()); y_pred.extend(preds.cpu().numpy())\n",
    "    avg_loss = running_loss / len(dataloader.dataset)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    return avg_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fdba53c1-67bf-44ca-a7a5-3a561b1d8a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(model, criterion, dataloader, class_names=None, log_to_wandb=True, prefix=\"val\"):\n",
    "    \"\"\"\n",
    "    Evaluate the model on a dataloader.\n",
    "    Logs metrics (loss, accuracy, precision, recall, F1) to W&B if enabled.\n",
    "\n",
    "    Args:\n",
    "        model: trained PyTorch model\n",
    "        criterion: loss function\n",
    "        dataloader: DataLoader to evaluate on\n",
    "        class_names: list of class names (optional, for confusion matrix)\n",
    "        log_to_wandb: whether to log metrics to W&B\n",
    "        prefix: \"val\" or \"test\" (used as key prefix in logs)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            logits = model(X)\n",
    "            loss = criterion(logits, y)\n",
    "            running_loss += loss.item() * X.size(0)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            y_true.extend(y.cpu().tolist())\n",
    "            y_pred.extend(preds.cpu().tolist())\n",
    "\n",
    "    avg_loss = running_loss / len(dataloader.dataset)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average=\"weighted\", zero_division=0\n",
    "    )\n",
    "\n",
    "    metrics = {\n",
    "        f\"{prefix}_loss\": avg_loss,\n",
    "        f\"{prefix}_accuracy\": acc,\n",
    "        f\"{prefix}_precision\": prec,\n",
    "        f\"{prefix}_recall\": rec,\n",
    "        f\"{prefix}_f1\": f1\n",
    "    }\n",
    "\n",
    "    # Log to W&B (safely)\n",
    "    if log_to_wandb:\n",
    "        wandb_log_safe(metrics)\n",
    "\n",
    "        # Confusion matrix\n",
    "        if class_names is not None:\n",
    "            cm = confusion_matrix(y_true, y_pred, labels=list(range(len(class_names))))\n",
    "            fig, ax = plt.subplots(figsize=(8, 6))\n",
    "            sns.heatmap(cm, annot=False, fmt=\"d\", cmap=\"Blues\",\n",
    "                        xticklabels=class_names, yticklabels=class_names, ax=ax)\n",
    "            ax.set_xlabel(\"Predicted\")\n",
    "            ax.set_ylabel(\"True\")\n",
    "            ax.set_title(f\"{prefix.capitalize()} Confusion Matrix\")\n",
    "            wandb_log_safe({f\"{prefix}_confusion_matrix\": wandb.Image(fig)})\n",
    "            plt.close(fig)\n",
    "\n",
    "    return avg_loss, acc, prec, rec, f1, y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0bb4e55f-8e58-4500-abed-131131d3f82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=7, mode=\"max\", min_delta=1e-4):\n",
    "        self.patience = patience\n",
    "        self.mode = mode\n",
    "        self.min_delta = min_delta\n",
    "        self.best = None\n",
    "        self.counter = 0\n",
    "    def step(self, value):\n",
    "        if self.best is None:\n",
    "            self.best = value; return False\n",
    "        improvement = value - self.best if self.mode==\"max\" else self.best - value\n",
    "        if improvement > self.min_delta:\n",
    "            self.best = value; self.counter = 0; return False\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a044a78-55fc-4e81-9f15-83295078ba36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(model, train_loader, val_loader, cfg, writer=None):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # --- Optimizer ---\n",
    "    if cfg.get(\"optimizer\", \"SGD\") == \"AdamW\":\n",
    "        optimizer = optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=cfg[\"lr\"],\n",
    "            weight_decay=cfg[\"weight_decay\"]\n",
    "        )\n",
    "    else:  # default SGD\n",
    "        optimizer = optim.SGD(\n",
    "            model.parameters(),\n",
    "            lr=cfg[\"lr\"],\n",
    "            momentum=cfg[\"momentum\"],\n",
    "            weight_decay=cfg[\"weight_decay\"]\n",
    "        )\n",
    "\n",
    "    # --- Scheduler ---\n",
    "    if cfg.get(\"scheduler\", \"StepLR\") == \"StepLR\":\n",
    "        scheduler = optim.lr_scheduler.StepLR(\n",
    "            optimizer,\n",
    "            step_size=cfg.get(\"step_size\", 10),\n",
    "            gamma=cfg.get(\"gamma\", 0.5)\n",
    "        )\n",
    "    elif cfg[\"scheduler\"] == \"Plateau\":\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode=\"min\",\n",
    "            factor=0.5,\n",
    "            patience=3\n",
    "        )\n",
    "    else:\n",
    "        scheduler = None\n",
    "\n",
    "    history = {\"train_loss\": [], \"val_loss\": [], \"train_acc\": [], \"val_acc\": [], \"lr\": []}\n",
    "    best_val_loss = float(\"inf\")\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(cfg[\"epochs\"]):\n",
    "        # --------------------\n",
    "        # Train\n",
    "        # --------------------\n",
    "        model.train()\n",
    "        train_loss, correct, total = 0.0, 0, 0\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "\n",
    "            if cfg.get(\"l1_lambda\", 0.0) > 0.0:\n",
    "                loss = loss + cfg[\"l1_lambda\"] * l1_penalty(model)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            # gradient clipping\n",
    "            if cfg.get(\"grad_clip\", None):\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), cfg[\"grad_clip\"])\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * xb.size(0)\n",
    "            _, predicted = preds.max(1)\n",
    "            correct += predicted.eq(yb).sum().item()\n",
    "            total += yb.size(0)\n",
    "\n",
    "        train_loss /= total\n",
    "        train_acc = correct / total\n",
    "\n",
    "        # --------------------\n",
    "        # Validate\n",
    "        # --------------------\n",
    "        model.eval()\n",
    "        val_loss, correct, total = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                preds = model(xb)\n",
    "                loss = criterion(preds, yb)\n",
    "                val_loss += loss.item() * xb.size(0)\n",
    "                _, predicted = preds.max(1)\n",
    "                correct += predicted.eq(yb).sum().item()\n",
    "                total += yb.size(0)\n",
    "        val_loss /= total\n",
    "        val_acc = correct / total\n",
    "\n",
    "        # --------------------\n",
    "        # Scheduler Step\n",
    "        # --------------------\n",
    "        if scheduler is not None:\n",
    "            if isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "\n",
    "        lr_curr = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "        # Save history\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "        history[\"lr\"].append(lr_curr)\n",
    "\n",
    "        # Logging\n",
    "        if writer:\n",
    "            writer.add_scalar(\"Loss/train\", train_loss, epoch)\n",
    "            writer.add_scalar(\"Loss/val\", val_loss, epoch)\n",
    "            writer.add_scalar(\"Accuracy/train\", train_acc, epoch)\n",
    "            writer.add_scalar(\"Accuracy/val\", val_acc, epoch)\n",
    "            writer.add_scalar(\"LR\", lr_curr, epoch)\n",
    "\n",
    "        wandb_log_safe({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"train_acc\": train_acc,\n",
    "            \"val_acc\": val_acc,\n",
    "            \"lr\": lr_curr,\n",
    "        })\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{cfg['epochs']} \"\n",
    "              f\"Train loss={train_loss:.4f}, acc={train_acc:.4f} \"\n",
    "              f\"Val loss={val_loss:.4f}, acc={val_acc:.4f} lr={lr_curr:.5f}\")\n",
    "\n",
    "        # --------------------\n",
    "        # Early stopping\n",
    "        # --------------------\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            best_model = model.state_dict()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= cfg[\"patience\"]:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(best_model)\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1685d9a-d5dc-4f40-ae3c-b31b0495a3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"# 6) Grid Search\n",
    "\n",
    "Brute-force search over learning rate, weight decay, and batch size on the 30% subset\n",
    "with small epochs for speed. Selects the best by validation accuracy.\n",
    "\"\"\"\n",
    "# -------------------------\n",
    "# 5) Grid Search (lr, weight_decay, batch_size)\n",
    "#    Quick & conservative (small epochs). We'll keep epochs small to save time for grid search.\n",
    "# -------------------------\n",
    "param_grid = {\n",
    "    \"lr\": [0.1, 0.01, 0.001],\n",
    "    \"weight_decay\": [5e-4, 1e-4, 0.0],\n",
    "    \"batch_size\": [64, 128]\n",
    "}\n",
    "grid_results = []\n",
    "grid_config = {\"epochs\": 12, \"momentum\": 0.9, \"l1_lambda\": 0.0, \"patience\": 4}\n",
    "\n",
    "print(\"Starting grid search on the 30% small subset (this will iterate over combinations).\")\n",
    "for lr in param_grid[\"lr\"]:\n",
    "    for wd in param_grid[\"weight_decay\"]:\n",
    "        for bs in param_grid[\"batch_size\"]:\n",
    "            cfg = {\"lr\": lr, \"weight_decay\": wd, \"momentum\": grid_config[\"momentum\"],\n",
    "                   \"epochs\": grid_config[\"epochs\"], \"l1_lambda\": grid_config[\"l1_lambda\"], \"patience\": grid_config[\"patience\"]}\n",
    "            train_loader, val_loader, _ = make_dataloaders(bs)\n",
    "            run_name = f\"grid_lr{lr}_wd{wd}_bs{bs}_{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "            writer = SummaryWriter(log_dir=os.path.join(OUT_DIR, \"runs\", run_name))\n",
    "            model = FeedForwardMLP(hidden_sizes=[2048,1024,512,256], dropout=0.5)\n",
    "            model, history = run_training(model, train_loader, val_loader, cfg, writer=writer)\n",
    "            best_val_acc = max(history[\"val_acc\"]) if history[\"val_acc\"] else 0.0\n",
    "            grid_results.append({\"params\": {\"lr\": lr, \"weight_decay\": wd, \"batch_size\": bs}, \"val_acc\": best_val_acc, \"history\": history})\n",
    "            writer.close()\n",
    "            print(f\"Grid combo lr={lr},wd={wd},bs={bs} -> best_val_acc={best_val_acc:.4f}\")\n",
    "\n",
    "# choose best\n",
    "best_entry = max(grid_results, key=lambda x: x[\"val_acc\"])\n",
    "best_params = best_entry[\"params\"]\n",
    "print(\"GRID SEARCH COMPLETE. Best params:\", best_params, \"val_acc:\", best_entry[\"val_acc\"])\n",
    "\n",
    "# save best hyperparams\n",
    "with open(os.path.join(OUT_DIR, \"best_hyperparams.json\"), \"w\") as f:\n",
    "    json.dump(best_params, f)\n",
    "\n",
    "# Optional W&B login via Colab userdata secret (set in Colab > User data)\n",
    "if 'userdata' in globals() and userdata is not None:\n",
    "    try:\n",
    "        api_key = userdata.get('wandb_api')\n",
    "        if api_key:\n",
    "            wandb.login(key=api_key)\n",
    "    except Exception as e:\n",
    "        print(\"W&B login skipped:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "057de6c4-e0da-49eb-94bf-8d4fe561dd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since I already calculated these parameters from a previous experiment I am simply declaring them here\n",
    "\n",
    "best_params = {'lr': 0.01, 'weight_decay': 0.0, 'batch_size': 64}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f23cdd94-47c9-427d-ba49-13cac632749b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33minnocent-ics-2025\u001b[0m (\u001b[33mnimbus-neuron\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/innocentchikwanda/Desktop/Grad School/Deep Learning/Prosit1/wandb/run-20250919_214352-xekugzwo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nimbus-neuron/cinic10-mlp/runs/xekugzwo' target=\"_blank\">best_small_SGD_Plateau_Momentum_6</a></strong> to <a href='https://wandb.ai/nimbus-neuron/cinic10-mlp' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nimbus-neuron/cinic10-mlp' target=\"_blank\">https://wandb.ai/nimbus-neuron/cinic10-mlp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nimbus-neuron/cinic10-mlp/runs/xekugzwo' target=\"_blank\">https://wandb.ai/nimbus-neuron/cinic10-mlp/runs/xekugzwo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/nimbus-neuron/cinic10-mlp/runs/xekugzwo?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x30b5a1d00>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# 7) Train with Best Params (Small Subset)\n",
    "\n",
    "Trains on the small subset using best hyperparameters; logs to W&B and evaluates on\n",
    "the small test slice.\n",
    "\"\"\"\n",
    "# -------------------------\n",
    "# 6) Train with best params + log to W&B\n",
    "# -------------------------\n",
    "bs = best_params[\"batch_size\"]\n",
    "train_loader, val_loader, small_test_loader = make_dataloaders(bs)\n",
    "\n",
    "wandb.init(\n",
    "    project=wandb_project,\n",
    "    name=f\"best_small_SGD_Plateau_Momentum_6\",\n",
    "    config={\n",
    "        \"batch_size\": bs,\n",
    "        \"lr\": best_params[\"lr\"],\n",
    "        #\"optimizer\": \"AdamW\",\n",
    "        \"scheduler\": \"StepLR\",\n",
    "        \"weight_decay\": best_params[\"weight_decay\"],\n",
    "        \"momentum\": 0.6,\n",
    "        \"epochs\": 100,\n",
    "        \"l1_lambda\": 0.0,\n",
    "        \"patience\": 6,\n",
    "        \"model\": \"FeedForwardMLP\",\n",
    "        \"hidden_sizes\": [2048,1024,512,256],\n",
    "        \"dropout\": 0.5\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ba7202-30e5-4984-8ed6-edbb0eb51afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FeedForwardMLP(hidden_sizes=[2048,1024,512,256], dropout=0.5)\n",
    "cfg = wandb.config\n",
    "model, history = run_training(model, train_loader, val_loader, cfg, writer=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8177f51c-40f6-4d68-a8d5-7a26dae9145c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on small test set (the 3rd slice of small subset)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "val_loss, val_acc, val_prec, val_rec, val_f1, y_true_small, y_pred_small = evaluate(\n",
    "    model, criterion, small_test_loader, class_names=CLASS_NAMES, prefix=\"small_test\"\n",
    ")\n",
    "print(\"Small test set results -> acc:\", val_acc, \"prec:\", val_prec, \"rec:\", val_rec, \"f1:\", val_f1)\n",
    "print(\"Confusion matrix (small test):\")\n",
    "print(confusion_matrix(y_true_small, y_pred_small))\n",
    "\n",
    "# Save small-test model\n",
    "torch.save(model.state_dict(), os.path.join(OUT_DIR, \"model_small_best.pth\"))\n",
    "\n",
    "# Record small-test metrics in W&B summary\n",
    "if wandb.run is not None:\n",
    "    try:\n",
    "        wandb.run.summary.update({\n",
    "            \"small_test_loss\": val_loss,\n",
    "            \"small_test_accuracy\": val_acc,\n",
    "            \"small_test_precision\": val_prec,\n",
    "            \"small_test_recall\": val_rec,\n",
    "            \"small_test_f1\": val_f1,\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(\"W&B summary update skipped:\", e)\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37fcd174-9403-4809-b6b3-4853fa5a402b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">best_small_SGD_Plateau_Momentum_6</strong> at: <a href='https://wandb.ai/nimbus-neuron/cinic10-mlp/runs/xekugzwo' target=\"_blank\">https://wandb.ai/nimbus-neuron/cinic10-mlp/runs/xekugzwo</a><br> View project at: <a href='https://wandb.ai/nimbus-neuron/cinic10-mlp' target=\"_blank\">https://wandb.ai/nimbus-neuron/cinic10-mlp</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250919_214352-xekugzwo/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d12fa1d5-81f9-4676-a1c6-da4fa6c7348b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training on combined train+valid (with augmentation) using best hyperparams.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/innocentchikwanda/Desktop/Grad School/Deep Learning/Prosit1/wandb/run-20250919_214716-3i0vycg2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nimbus-neuron/cinic10-mlp/runs/3i0vycg2' target=\"_blank\">final_combined_20250919214716</a></strong> to <a href='https://wandb.ai/nimbus-neuron/cinic10-mlp' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nimbus-neuron/cinic10-mlp' target=\"_blank\">https://wandb.ai/nimbus-neuron/cinic10-mlp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nimbus-neuron/cinic10-mlp/runs/3i0vycg2' target=\"_blank\">https://wandb.ai/nimbus-neuron/cinic10-mlp/runs/3i0vycg2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 Train loss=1.9368, acc=0.2843 Val loss=1.7749, acc=0.3558 lr=0.01000\n",
      "Epoch 2/50 Train loss=1.8150, acc=0.3394 Val loss=1.6963, acc=0.3799 lr=0.01000\n",
      "Epoch 3/50 Train loss=1.7797, acc=0.3546 Val loss=1.6959, acc=0.3884 lr=0.01000\n",
      "Epoch 4/50 Train loss=1.7619, acc=0.3637 Val loss=1.6784, acc=0.3976 lr=0.01000\n",
      "Epoch 5/50 Train loss=1.7472, acc=0.3681 Val loss=1.6719, acc=0.4001 lr=0.01000\n",
      "Epoch 6/50 Train loss=1.7373, acc=0.3718 Val loss=1.6479, acc=0.4044 lr=0.01000\n",
      "Epoch 7/50 Train loss=1.7338, acc=0.3738 Val loss=1.6447, acc=0.4104 lr=0.01000\n",
      "Epoch 8/50 Train loss=1.7279, acc=0.3784 Val loss=1.6457, acc=0.4094 lr=0.01000\n",
      "Epoch 9/50 Train loss=1.7233, acc=0.3787 Val loss=1.6539, acc=0.4082 lr=0.01000\n",
      "Epoch 10/50 Train loss=1.7193, acc=0.3810 Val loss=1.6651, acc=0.4018 lr=0.00500\n",
      "Epoch 11/50 Train loss=1.6753, acc=0.3964 Val loss=1.5960, acc=0.4304 lr=0.00500\n",
      "Epoch 12/50 Train loss=1.6582, acc=0.4041 Val loss=1.5870, acc=0.4325 lr=0.00500\n",
      "Epoch 13/50 Train loss=1.6470, acc=0.4070 Val loss=1.5845, acc=0.4349 lr=0.00500\n",
      "Epoch 14/50 Train loss=1.6384, acc=0.4111 Val loss=1.5683, acc=0.4421 lr=0.00500\n",
      "Epoch 15/50 Train loss=1.6311, acc=0.4140 Val loss=1.5634, acc=0.4440 lr=0.00500\n",
      "Epoch 16/50 Train loss=1.6252, acc=0.4172 Val loss=1.5466, acc=0.4473 lr=0.00500\n",
      "Epoch 17/50 Train loss=1.6235, acc=0.4163 Val loss=1.5495, acc=0.4494 lr=0.00500\n",
      "Epoch 18/50 Train loss=1.6177, acc=0.4185 Val loss=1.5390, acc=0.4526 lr=0.00500\n",
      "Epoch 19/50 Train loss=1.6115, acc=0.4211 Val loss=1.5211, acc=0.4577 lr=0.00500\n",
      "Epoch 20/50 Train loss=1.6060, acc=0.4234 Val loss=1.5338, acc=0.4556 lr=0.00250\n",
      "Epoch 21/50 Train loss=1.5858, acc=0.4299 Val loss=1.5159, acc=0.4616 lr=0.00250\n",
      "Epoch 22/50 Train loss=1.5749, acc=0.4359 Val loss=1.5149, acc=0.4637 lr=0.00250\n",
      "Epoch 23/50 Train loss=1.5686, acc=0.4376 Val loss=1.5019, acc=0.4658 lr=0.00250\n",
      "Epoch 24/50 Train loss=1.5618, acc=0.4389 Val loss=1.4972, acc=0.4655 lr=0.00250\n",
      "Epoch 25/50 Train loss=1.5597, acc=0.4402 Val loss=1.4930, acc=0.4700 lr=0.00250\n",
      "Epoch 26/50 Train loss=1.5570, acc=0.4418 Val loss=1.4932, acc=0.4710 lr=0.00250\n",
      "Epoch 27/50 Train loss=1.5502, acc=0.4446 Val loss=1.4876, acc=0.4704 lr=0.00250\n",
      "Epoch 28/50 Train loss=1.5487, acc=0.4452 Val loss=1.4757, acc=0.4741 lr=0.00250\n",
      "Epoch 29/50 Train loss=1.5457, acc=0.4461 Val loss=1.4751, acc=0.4741 lr=0.00250\n",
      "Epoch 30/50 Train loss=1.5411, acc=0.4484 Val loss=1.4665, acc=0.4785 lr=0.00125\n",
      "Epoch 31/50 Train loss=1.5318, acc=0.4514 Val loss=1.4627, acc=0.4797 lr=0.00125\n",
      "Epoch 32/50 Train loss=1.5222, acc=0.4545 Val loss=1.4528, acc=0.4826 lr=0.00125\n",
      "Epoch 33/50 Train loss=1.5181, acc=0.4562 Val loss=1.4543, acc=0.4834 lr=0.00125\n",
      "Epoch 34/50 Train loss=1.5165, acc=0.4574 Val loss=1.4490, acc=0.4815 lr=0.00125\n",
      "Epoch 35/50 Train loss=1.5160, acc=0.4574 Val loss=1.4458, acc=0.4859 lr=0.00125\n",
      "Epoch 36/50 Train loss=1.5122, acc=0.4575 Val loss=1.4442, acc=0.4841 lr=0.00125\n",
      "Epoch 37/50 Train loss=1.5124, acc=0.4584 Val loss=1.4438, acc=0.4865 lr=0.00125\n",
      "Epoch 38/50 Train loss=1.5087, acc=0.4605 Val loss=1.4433, acc=0.4867 lr=0.00125\n",
      "Epoch 39/50 Train loss=1.5057, acc=0.4606 Val loss=1.4339, acc=0.4890 lr=0.00125\n",
      "Epoch 40/50 Train loss=1.5048, acc=0.4611 Val loss=1.4403, acc=0.4885 lr=0.00063\n",
      "Epoch 41/50 Train loss=1.4984, acc=0.4628 Val loss=1.4292, acc=0.4911 lr=0.00063\n",
      "Epoch 42/50 Train loss=1.4965, acc=0.4640 Val loss=1.4260, acc=0.4920 lr=0.00063\n",
      "Epoch 43/50 Train loss=1.4940, acc=0.4649 Val loss=1.4249, acc=0.4923 lr=0.00063\n",
      "Epoch 44/50 Train loss=1.4920, acc=0.4655 Val loss=1.4249, acc=0.4932 lr=0.00063\n",
      "Epoch 45/50 Train loss=1.4897, acc=0.4663 Val loss=1.4217, acc=0.4936 lr=0.00063\n",
      "Epoch 46/50 Train loss=1.4898, acc=0.4664 Val loss=1.4214, acc=0.4936 lr=0.00063\n",
      "Epoch 47/50 Train loss=1.4874, acc=0.4668 Val loss=1.4218, acc=0.4953 lr=0.00063\n",
      "Epoch 48/50 Train loss=1.4866, acc=0.4684 Val loss=1.4192, acc=0.4943 lr=0.00063\n",
      "Epoch 49/50 Train loss=1.4854, acc=0.4663 Val loss=1.4192, acc=0.4944 lr=0.00063\n",
      "Epoch 50/50 Train loss=1.4871, acc=0.4677 Val loss=1.4175, acc=0.4958 lr=0.00031\n",
      "Held-out TEST set metrics -> acc:0.4732 prec:0.4810 rec:0.4732 f1:0.4681\n",
      "Confusion matrix (test):\n",
      "[[5177  328  610  202  339   50  132  279 1386  497]\n",
      " [ 326 4689  205  437  240   47  276  270  773 1737]\n",
      " [ 638  173 3478 1173 1087  322 1187  422  408  112]\n",
      " [  70  136  804 4126  913  661 1347  456  310  177]\n",
      " [ 312  127  975 1136 3455  279 1049 1091  408  168]\n",
      " [ 137  132  970 3021 1101 1445  864  809  350  171]\n",
      " [  42  120  768 1356  638   94 5595  144  148   95]\n",
      " [ 206   96  423  933 1236  319  231 5020  255  281]\n",
      " [ 831  484  445  492  401   62  207  238 5252  588]\n",
      " [ 427 1755  217  433  248   57  257  499  758 4349]]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'val_acc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 67\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Save final model and summary\u001b[39;00m\n\u001b[1;32m     64\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(final_model\u001b[38;5;241m.\u001b[39mstate_dict(), os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(OUT_DIR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_model_combined.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     65\u001b[0m summary \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_hyperparams\u001b[39m\u001b[38;5;124m\"\u001b[39m: best_params,\n\u001b[0;32m---> 67\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msmall_test_metrics\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124macc\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mval_acc\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprec\u001b[39m\u001b[38;5;124m\"\u001b[39m: val_prec, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrec\u001b[39m\u001b[38;5;124m\"\u001b[39m: val_rec, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m\"\u001b[39m: val_f1},\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_test_metrics\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124macc\u001b[39m\u001b[38;5;124m\"\u001b[39m: test_acc, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprec\u001b[39m\u001b[38;5;124m\"\u001b[39m: test_prec, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrec\u001b[39m\u001b[38;5;124m\"\u001b[39m: test_rec, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m\"\u001b[39m: test_f1}\n\u001b[1;32m     69\u001b[0m }\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(OUT_DIR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_summary.json\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     71\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(summary, f, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'val_acc' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"# 8) Final Training on Combined Train+Valid\n",
    "\n",
    "Combine train and valid (optionally with augmentation) and train the final model.\n",
    "Validate on the original valid set and test on the original test split.\n",
    "\"\"\"\n",
    "# -------------------------\n",
    "# 7) After success: Combine train + valid (augment both) for final training; evaluate on held-out test (original test split)\n",
    "# -------------------------\n",
    "print(\"Final training on combined train+valid (with augmentation) using best hyperparams.\")\n",
    "final_train_loader = make_full_train_loader(batch_size=best_params[\"batch_size\"], augment=True)\n",
    "final_cfg = {\"lr\": best_params[\"lr\"], \"weight_decay\": best_params[\"weight_decay\"], \"momentum\": 0.9, \"epochs\": 50, \"l1_lambda\": 0.0, \"patience\": 8}\n",
    "tb_name = f\"final_combined_{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "writer = SummaryWriter(log_dir=os.path.join(OUT_DIR, \"runs\", tb_name))\n",
    "wandb.init(\n",
    "    project=wandb_project,\n",
    "    name=tb_name,\n",
    "    config={\n",
    "        **final_cfg,\n",
    "        \"batch_size\": best_params[\"batch_size\"],\n",
    "        \"model\": \"FeedForwardMLP\",\n",
    "        \"hidden_sizes\": [2048,1024,512,256],\n",
    "        \"dropout\": 0.5,\n",
    "        \"phase\": \"final_combined\",\n",
    "        \"lr\": best_params[\"lr\"],\n",
    "        #\"optimizer\": \"AdamW\",\n",
    "        \"scheduler\": \"StepLR\",\n",
    "        \"weight_decay\": best_params[\"weight_decay\"],\n",
    "        \"momentum\": 0.6,\n",
    "        \"epochs\": 100,\n",
    "        \"l1_lambda\": 0.0,\n",
    "        \"patience\": 6\n",
    "    }\n",
    ")\n",
    "\n",
    "final_model = FeedForwardMLP(hidden_sizes=[2048,1024,512,256], dropout=0.5)\n",
    "final_model, final_history = run_training(final_model, final_train_loader, valid_loader_full, final_cfg, writer=writer)\n",
    "writer.close()\n",
    "\n",
    "# Evaluate on held-out test (original test split)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "test_loss, test_acc, test_prec, test_rec, test_f1, y_true_test, y_pred_test = evaluate(\n",
    "    final_model, criterion, test_loader_full, class_names=CLASS_NAMES, prefix=\"test\"\n",
    ")\n",
    "\n",
    "val_loss, val_acc, val_prec, val_rec, val_f1, y_true_small, y_pred_small = evaluate(\n",
    "    model, criterion, small_test_loader, class_names=CLASS_NAMES, prefix=\"small_test\"\n",
    ")\n",
    "print(\"Held-out TEST set metrics -> acc:%.4f prec:%.4f rec:%.4f f1:%.4f\" % (test_acc, test_prec, test_rec, test_f1))\n",
    "print(\"Confusion matrix (test):\")\n",
    "print(confusion_matrix(y_true_test, y_pred_test))\n",
    "\n",
    "# Record final test metrics in W&B summary\n",
    "if wandb.run is not None:\n",
    "    try:\n",
    "        wandb.run.summary.update({\n",
    "            \"test_loss\": test_loss,\n",
    "            \"test_accuracy\": test_acc,\n",
    "            \"test_precision\": test_prec,\n",
    "            \"test_recall\": test_rec,\n",
    "            \"test_f1\": test_f1,\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(\"W&B summary update skipped:\", e)\n",
    "\n",
    "\n",
    "\n",
    "# Save final model and summary\n",
    "torch.save(final_model.state_dict(), os.path.join(OUT_DIR, \"final_model_combined.pth\"))\n",
    "summary = {\n",
    "    \"best_hyperparams\": best_params,\n",
    "    \"small_test_metrics\": {\"acc\": val_acc, \"prec\": val_prec, \"rec\": val_rec, \"f1\": val_f1},\n",
    "    \"final_test_metrics\": {\"acc\": test_acc, \"prec\": test_prec, \"rec\": test_rec, \"f1\": test_f1}\n",
    "}\n",
    "with open(os.path.join(OUT_DIR, \"training_summary.json\"), \"w\") as f:\n",
    "    json.dump(summary, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66162fcb-bb17-422e-860f-470c87a21534",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# 9) Final Metrics & Plots\n",
    "\n",
    "Plot training curves (loss/acc/lr) and print a classification report.\n",
    "\"\"\"\n",
    "# -------------------------\n",
    "# 8) Final metrics & plots (loss/acc/LR) saved & shown\n",
    "# -------------------------\n",
    "def plot_history(hist, title_prefix=\"history\", save_path=None):\n",
    "    epochs = range(1, len(hist[\"train_loss\"])+1)\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.plot(epochs, hist[\"train_loss\"], label=\"train_loss\")\n",
    "    plt.plot(epochs, hist[\"val_loss\"], label=\"val_loss\")\n",
    "    plt.title(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.plot(epochs, hist[\"train_acc\"], label=\"train_acc\")\n",
    "    plt.plot(epochs, hist[\"val_acc\"], label=\"val_acc\")\n",
    "    plt.title(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.plot(epochs, hist[\"lr\"], label=\"lr\")\n",
    "    plt.title(\"Learning Rate\")\n",
    "    plt.legend()\n",
    "    plt.suptitle(title_prefix)\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "    plt.show()\n",
    "\n",
    "plot_history(final_history, title_prefix=\"Final Combined Training History\", save_path=os.path.join(OUT_DIR, \"final_training_history.png\"))\n",
    "\n",
    "# Display final classification report on held-out test\n",
    "print(\"FINAL CLASSIFICATION REPORT (held-out test):\")\n",
    "print(classification_report(y_true_test, y_pred_test, digits=4))\n",
    "\n",
    "print(\"Artifacts saved to:\", OUT_DIR)\n",
    "print(\"To inspect TensorBoard in Colab run:\")\n",
    "print(\"  %load_ext tensorboard\")\n",
    "print(f\"  %tensorboard --logdir {os.path.join(OUT_DIR,'runs')}\")\n",
    "\n",
    "wandb.finish()\n",
    "\n",
    "# Done.\n",
    "\n",
    "\"\"\"Dont Run\"\"\"\n",
    "\n",
    "# -------------------------\n",
    "# 8) (Optional) Train with FULL training set (train+valid+test) IF you want a final model for production.\n",
    "#    NOTE: training on test removes your held-out evaluation. Save separately if you do this.\n",
    "# -------------------------\n",
    "# If you want to create a \"production\" model trained on all labeled data:\n",
    "\n",
    "\"\"\"\n",
    "full_all = ConcatDataset([datasets.ImageFolder(os.path.join(DATA_DIR, \"train\"), transform=train_transform_aug),\n",
    "                          datasets.ImageFolder(os.path.join(DATA_DIR, \"valid\"), transform=train_transform_aug),\n",
    "                          datasets.ImageFolder(os.path.join(DATA_DIR, \"test\"),  transform=train_transform_aug)])\n",
    "full_all_loader = DataLoader(full_all, batch_size=best_params[\"batch_size\"], shuffle=True, num_workers=2, pin_memory=True)\n",
    "\n",
    "prod_model = FeedForwardMLP(hidden_sizes=[2048,1024,512,256], dropout=0.5).to(device)\n",
    "prod_cfg = {\"lr\": best_params[\"lr\"], \"weight_decay\": best_params[\"weight_decay\"], \"momentum\": 0.9, \"epochs\": 30, \"l1_lambda\": 0.0, \"patience\": 6}\n",
    "tb_name = f\"prod_all_{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "writer = SummaryWriter(log_dir=os.path.join(OUT_DIR, \"runs\", tb_name))\n",
    "prod_model, prod_hist = run_training(prod_model, full_all_loader, valid_loader_full, prod_cfg, writer=writer)\n",
    "writer.close()\n",
    "torch.save(prod_model.state_dict(), os.path.join(OUT_DIR, \"prod_model_all_data.pth\"))\n",
    "\n",
    "\"\"\"\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
